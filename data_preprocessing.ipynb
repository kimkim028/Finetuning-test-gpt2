{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercasing the text except for the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      flags                                        instruction  category  \\\n",
      "0       BLZ           i am tring to see the early exit penalty    CANCEL   \n",
      "1        BK                             check cancellation fee    CANCEL   \n",
      "2       BEL  i do not know how to check the cancellation ch...    CANCEL   \n",
      "3       BLM             i want help to see the early exit fees    CANCEL   \n",
      "4        BL                            checking early exit fee    CANCEL   \n",
      "...     ...                                                ...       ...   \n",
      "2845    BLW  information about correcting the damn shipping...  SHIPPING   \n",
      "2846    BLM              i have troubles correcting my address  SHIPPING   \n",
      "2847  BLMQZ  i have errors trying to edit the shippikng add...  SHIPPING   \n",
      "2848    BIL                      how can i modify the address?  SHIPPING   \n",
      "2849    BEL    why i cannot enter a new shipping address valid  SHIPPING   \n",
      "\n",
      "                       intent  \\\n",
      "0      check_cancellation_fee   \n",
      "1      check_cancellation_fee   \n",
      "2      check_cancellation_fee   \n",
      "3      check_cancellation_fee   \n",
      "4      check_cancellation_fee   \n",
      "...                       ...   \n",
      "2845  change_shipping_address   \n",
      "2846  change_shipping_address   \n",
      "2847  change_shipping_address   \n",
      "2848  change_shipping_address   \n",
      "2849  set_up_shipping_address   \n",
      "\n",
      "                                               response  \n",
      "0     certainly! to view the early exit penalty, you...  \n",
      "1     certainly! let me find the information regardi...  \n",
      "2     no problem! i can assist you with checking the...  \n",
      "3     sure! i'd be happy to assist you in checking t...  \n",
      "4     i'll do my best! i can help you with checking ...  \n",
      "...                                                 ...  \n",
      "2845  assuredly! i understand your frustration with ...  \n",
      "2846  i apologize for the difficulty you're facing i...  \n",
      "2847  i apologize for the inconvenience. to assist y...  \n",
      "2848  to modify your shipping address, you can eithe...  \n",
      "2849  i understand your frustration regarding the in...  \n",
      "\n",
      "[2850 rows x 5 columns]\n",
      "asdasdsa\n"
     ]
    }
   ],
   "source": [
    "# Function to lowercase text except for placeholders\n",
    "def lowercase_placeholders(text):\n",
    "    # Step 1: Find placeholders (entities)\n",
    "    placeholders = re.findall(r'\\{\\{.*?\\}\\}', text)\n",
    "    \n",
    "    # Step 2: Replace placeholders with temporary markers\n",
    "    temp_markers = {placeholder: f\"PLACEHOLDER_{i}\" for i, placeholder in enumerate(placeholders)}\n",
    "    \n",
    "    for placeholder, temp_marker in temp_markers.items():\n",
    "        text = text.replace(placeholder, temp_marker)\n",
    "    \n",
    "    # Step 3: Lowercase the text\n",
    "    lowercased_text = text.lower()\n",
    "\n",
    "    # Step 4: Restore placeholders\n",
    "    for placeholder, temp_marker in temp_markers.items():\n",
    "        lowercased_text = lowercased_text.replace(temp_marker.lower(), placeholder)\n",
    "    \n",
    "    return lowercased_text\n",
    "\n",
    "df = pd.read_csv('sampled_dataset.csv')\n",
    "\n",
    "#convert instruction to lowercase\n",
    "df['instruction'] = df['instruction'].apply(lowercase_placeholders)\n",
    "\n",
    "#convert response to lowercase\n",
    "df['response'] = df['response'].apply(lowercase_placeholders)\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(\"asdasdsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "27\n",
      "category\n",
      "CANCEL      950\n",
      "ORDER       950\n",
      "SHIPPING    950\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_7596\\1423447274.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = filtered_df.groupby('category').apply(lambda x: x.sample(n=samples_per_category, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# # Get the unique categories\n",
    "# categories = df['category'].unique()\n",
    "# print(len(categories))\n",
    "\n",
    "# # Get the unique intents\n",
    "# intents = df['intent'].unique()\n",
    "# print(len(intents))\n",
    "\n",
    "# # Select the first 3 categories\n",
    "# selected_categories = categories[:3]\n",
    "\n",
    "# # Filter the DataFrame to include only the selected categories\n",
    "# filtered_df = df[df['category'].isin(selected_categories)]\n",
    "\n",
    "# # Define the number of samples per category\n",
    "# samples_per_category = 950  # Adjust this number as needed\n",
    "\n",
    "# # Group by category and sample\n",
    "# sampled_df = filtered_df.groupby('category').apply(lambda x: x.sample(n=samples_per_category, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# # Check the distribution of the sampled data\n",
    "# print(sampled_df['category'].value_counts())\n",
    "\n",
    "# # Save the sampled dataset to a new CSV file\n",
    "# sampled_df.to_csv('sampled_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data in the sampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'i am tring to see the early exit penalty', 'category': 'CANCEL', 'intent': 'check_cancellation_fee'}\n",
      "{'CANCEL': 0, 'ORDER': 1, 'SHIPPING': 2}\n",
      "{'check_cancellation_fee': 0, 'change_order': 1, 'place_order': 2, 'cancel_order': 3, 'track_order': 4, 'change_shipping_address': 5, 'set_up_shipping_address': 6}\n",
      "{'text': 'i forgot to include an item in purchase {{Order Number}}', 'category': 1, 'intent': 1}\n",
      "{'text': 'assistance checking the termination fees', 'category': 0, 'intent': 0}\n",
      "{'text': 'want help modifying my address', 'category': 2, 'intent': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert to list of dictionaries for multi-task classification\n",
    "data = [{\"text\": row['instruction'], \"category\": row['category'], \"intent\": row['intent']} for _, row in df.iterrows()]\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train data into train and validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# Optional: Encode both the category and intent labels to numerical values\n",
    "category_encoder = {category: idx for idx, category in enumerate(df['category'].unique())}\n",
    "intent_encoder = {intent: idx for idx, intent in enumerate(df['intent'].unique())}\n",
    "\n",
    "# Encode labels in both category and intent\n",
    "train_data = [{\"text\": entry['text'], \n",
    "               \"category\": category_encoder[entry['category']], \n",
    "               \"intent\": intent_encoder[entry['intent']]} for entry in train_data]\n",
    "\n",
    "val_data = [{\"text\": entry['text'], \n",
    "             \"category\": category_encoder[entry['category']], \n",
    "             \"intent\": intent_encoder[entry['intent']]} for entry in val_data]\n",
    "\n",
    "test_data = [{\"text\": entry['text'], \n",
    "              \"category\": category_encoder[entry['category']], \n",
    "              \"intent\": intent_encoder[entry['intent']]} for entry in test_data]\n",
    "\n",
    "print(data[0])\n",
    "print(category_encoder)\n",
    "print(intent_encoder)\n",
    "\n",
    "print(train_data[0])\n",
    "print(val_data[0])\n",
    "print(test_data[20])\n",
    "print(\"sadasdasdas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Multi task gpt2 classification collator for intent and category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskGpt2ClassificationCollator:\n",
    "    def __init__(self, use_tokenizer, category_encoder, intent_encoder, max_sequence_len=None):\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        self.max_sequence_len = max_sequence_len or use_tokenizer.model_max_length\n",
    "        self.category_encoder = category_encoder\n",
    "        self.intent_encoder = intent_encoder\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        # Extract texts, category labels, and intent labels from the batch\n",
    "        texts = [item['text'] for item in batch]\n",
    "        category_labels = [item['category'] for item in batch]\n",
    "        intent_labels = [item['intent'] for item in batch]\n",
    "        \n",
    "        # Tokenize the texts\n",
    "        inputs = self.use_tokenizer(\n",
    "            texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=self.max_sequence_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Convert category and intent labels to tensors\n",
    "        inputs.update({\n",
    "            'category_labels': torch.tensor(category_labels),\n",
    "            'intent_labels': torch.tensor(intent_labels)\n",
    "        })\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize the collator for multi-task classification\n",
    "collator = MultiTaskGpt2ClassificationCollator(\n",
    "    use_tokenizer=tokenizer, \n",
    "    category_encoder=category_encoder,\n",
    "    intent_encoder=intent_encoder,\n",
    "    max_sequence_len=512\n",
    ")\n",
    "\n",
    "# Create DataLoader for both train and validation sets\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, collate_fn=collator)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, collate_fn=collator)\n",
    "val_dataloader = DataLoader(val_data, batch_size=32, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "True\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "asdas\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # This will print the version of PyTorch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0)) \n",
    "\n",
    "print(\"asdas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
